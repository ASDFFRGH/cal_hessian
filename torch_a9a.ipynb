{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5056f760",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorchによる学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cd749dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader \n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import MNIST, CIFAR10, FashionMNIST\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4907e7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': '(32561, 123), (32561,)', 'val': '', 'test': '(16281, 122), (16281,)'}\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "float64\n",
      "-1.0\n",
      "<class 'numpy.float64'>\n",
      "torch.Size([32561, 123])\n",
      "torch.Size([16281, 123])\n"
     ]
    }
   ],
   "source": [
    "import dsdl\n",
    "\n",
    "ds = dsdl.load('a9a')\n",
    "print(ds)\n",
    "\n",
    "X_train, t_train = ds.get_train()\n",
    "X_test, t_test = ds.get_test()\n",
    "\n",
    "print(type(X_test))\n",
    "\n",
    "X_train = X_train.toarray()\n",
    "print(X_train.dtype)\n",
    "X_test = X_test.toarray()\n",
    "X_test = np.block([X_test, np.zeros((16281, 1))])\n",
    "\n",
    "print(t_train[1])\n",
    "print(type(t_train[1]))\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32)).clone()\n",
    "t_train = torch.from_numpy(t_train.astype(np.float32)).clone()\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32)).clone()\n",
    "t_test = torch.from_numpy(t_test.astype(np.float32)).clone()\n",
    "\n",
    "ds_train = TensorDataset(X_train, t_train) \n",
    "ds_test = TensorDataset(X_test, t_test)\n",
    "\n",
    "loader_train = DataLoader(ds_train, batch_size = 128, shuffle = True)\n",
    "loader_test = DataLoader(ds_test, batch_size = 128, shuffle = True)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "#X_train = train.data.type(torch.float32)\n",
    "#t_train = train.targets\n",
    "\n",
    "#print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7787de59",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class configs():\n",
    "    noise_type: str\n",
    "    dataset: str\n",
    "    learning_rate: float\n",
    "    batch_size: int\n",
    "    epochs: int = 10       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5094423",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7748d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_xavier_init(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.xavier_normal_(param)\n",
    "        if 'bias' in name:\n",
    "            nn.init.constant_(param, val=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f6d81bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_root(configs):\n",
    "    root = 'logs/log_' + configs.dataset + '_' + configs.noise_type\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7980ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_param(model, loss, lr):\n",
    "    with torch.no_grad():\n",
    "        for params in model.parameters():\n",
    "            params.data -= lr * params.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b12ea2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(root):\n",
    "    i = 1\n",
    "    while True:\n",
    "        path = root + '_' + str(i)\n",
    "        is_file = os.path.isfile(path)\n",
    "        if is_file:\n",
    "            i += 1\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    return path\n",
    "\n",
    "#root = 'log_mnist_finite'\n",
    "#path = get_path(root)\n",
    "#print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee0d695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_in = 28*28\n",
    "n_in = 123\n",
    "n_mid = 216\n",
    "#n_out = 10\n",
    "n_out = 1\n",
    "\n",
    "class MFNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MFNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu = nn.Sequential(\n",
    "            nn.Linear(n_in, n_mid),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_mid, n_mid),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_mid, n_out),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu(x)\n",
    "        return logits/n_mid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab95998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(epoch):\n",
    "    if epoch < 120:\n",
    "        return 0.2**0\n",
    "    elif epoch < 160:\n",
    "        return 0.2**1\n",
    "    elif epoch < 200:\n",
    "        return 0.2**2\n",
    "    else:\n",
    "        return 0.2**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7cadda00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#root = 'logs/log_mnist_sample'\n",
    "config = configs('finite', 'a9a', 0.1, 128)\n",
    "root = make_root(config)\n",
    "path = get_path(root)\n",
    "\n",
    "        \n",
    "def main(model, opt, loss_fn, scheduler, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        with open(path, 'a') as f:\n",
    "            f.write(f'EPOCH: {epoch}\\n')\n",
    "        print(f'EPOCH: {epoch}')\n",
    "        \n",
    "        start = time.time() \n",
    "        \n",
    "        #start train loop----------------------\n",
    "        train_loss = []\n",
    "        total_train = 0\n",
    "        correct_train = 0\n",
    "        \n",
    "        model.train()\n",
    "        for X, t in tqdm(loader_train):\n",
    "            X, t = X.to(device), t.to(device)\n",
    "            y = model(X)\n",
    "            #print(y)\n",
    "            t = t.to(torch.float32)\n",
    "            y = y.to(torch.float32)\n",
    "            t = t.view(-1, 1)\n",
    "            loss = loss_fn(y, t)\n",
    "            #print(loss)\n",
    "            opt.zero_grad() \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "            pred = y.argmax(1)\n",
    "            train_loss.append(loss.tolist())\n",
    "            \n",
    "            total_train += t.shape[0]\n",
    "            correct_train += (pred==t).sum().item()\n",
    "        scheduler.step()\n",
    "    \n",
    "        end  = time.time()\n",
    "        \n",
    "        log = f'train loss: {np.mean(train_loss):.3f}, accuracy: {correct_train/total_train:.3f}'\n",
    "        with open(path, 'a') as f:\n",
    "            f.write(log + f'time: {end - start:.5f}' + '\\n')\n",
    "        print(log)\n",
    "        #end train loop------------------------------------\n",
    "        \n",
    "        #start test loop-----------------------------------\n",
    "        test_loss = []\n",
    "        total_test= 0\n",
    "        correct_test = 0\n",
    "    \n",
    "        model.eval()\n",
    "        for X, t in loader_test:\n",
    "            X, t = X.to(device), t.to(device)\n",
    "            y = model(X)\n",
    "            t = t.to(torch.float32)\n",
    "            y = y.to(torch.float32)\n",
    "            t = t.view(-1, 1)            \n",
    "            loss = loss_fn(y, t)\n",
    "            pred = y.argmax(1)\n",
    "            #print(pred)\n",
    "            test_loss.append(loss.tolist())\n",
    "            \n",
    "            total_test += t.shape[0]\n",
    "            correct_test += (pred==t).sum().item()\n",
    "    \n",
    "        log = f'test loss: {np.mean(test_loss):.3f}, accuracy: {correct_test/total_test:.3f}'\n",
    "        with open(path, 'a') as f:\n",
    "            f.write(log + '\\n')\n",
    "        print(log)\n",
    "        # end test loop-------------------------------------- \n",
    "        \n",
    "        print(f'time: {end - start:.5f}')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "41bf5aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "EPOCH: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 255/255 [00:00<00:00, 368.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "train loss: 1.000, accuracy: 0.000\n",
      "test loss: 1.000, accuracy: 0.000\n",
      "time: 0.69373\n",
      "EPOCH: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 255/255 [00:00<00:00, 369.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "train loss: 1.000, accuracy: 0.000\n",
      "test loss: 1.000, accuracy: 0.000\n",
      "time: 0.69050\n",
      "EPOCH: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 255/255 [00:00<00:00, 380.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "train loss: 1.000, accuracy: 0.000\n",
      "test loss: 1.000, accuracy: 0.000\n",
      "time: 0.67176\n",
      "EPOCH: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 255/255 [00:00<00:00, 382.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "train loss: 1.000, accuracy: 0.000\n",
      "test loss: 1.000, accuracy: 0.000\n",
      "time: 0.66824\n",
      "EPOCH: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 255/255 [00:00<00:00, 377.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "train loss: 1.000, accuracy: 0.000\n",
      "test loss: 1.000, accuracy: 0.000\n",
      "time: 0.67629\n",
      "EPOCH: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 255/255 [00:00<00:00, 381.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "train loss: 1.000, accuracy: 0.000\n",
      "test loss: 1.000, accuracy: 0.000\n",
      "time: 0.66995\n",
      "EPOCH: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 255/255 [00:00<00:00, 381.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "train loss: 1.000, accuracy: 0.000\n",
      "test loss: 1.000, accuracy: 0.000\n",
      "time: 0.67032\n",
      "EPOCH: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 255/255 [00:00<00:00, 379.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "train loss: 1.000, accuracy: 0.000\n",
      "test loss: 1.000, accuracy: 0.000\n",
      "time: 0.67240\n",
      "EPOCH: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 255/255 [00:00<00:00, 379.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "train loss: 1.000, accuracy: 0.000\n",
      "test loss: 1.000, accuracy: 0.000\n",
      "time: 0.67239\n",
      "EPOCH: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 255/255 [00:00<00:00, 380.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "train loss: 1.000, accuracy: 0.000\n",
      "test loss: 1.000, accuracy: 0.000\n",
      "time: 0.67142\n"
     ]
    }
   ],
   "source": [
    "#configs = configs('finite', 'a9a', 0.1, 128)\n",
    "\n",
    "model = MFNN().to(device)\n",
    "param_xavier_init(model)\n",
    "opt = optim.SGD(model.parameters(), lr = 0.01, weight_decay = 0.01)\n",
    "#loss_fn = nn.BCELoss()\n",
    "loss_fn = nn.MSELoss()\n",
    "scheduler = optim.lr_scheduler.LambdaLR(opt, lr_lambda = func, verbose=True)\n",
    "epochs = 10\n",
    "main(model, opt, loss_fn, scheduler, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ff6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
